[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RLNotes",
    "section": "",
    "text": "Preface\nReinforced Learning is learning what to do -how to map situations to actions- so as maximize a numerical reard signal.\nWe formalize the problem of reinforcement learning using ideas from dynamical systems theory, specially, as the optimal control of incompletely-known Markov decission processes.\nOne of the challenges that arise in reinforcement learning, and not in other kinds of learning, is the trade-off betweeen exploration and exploitation. The agent has."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Elements of Reinforcement Learning.",
    "section": "",
    "text": "2 An Extended Example: Tic-Tac-Toe\nConsider the familiar child’s game of tic-tac-toe. Two players take turns playing on a three-by-three board. One player plays Xs and the other Os until one player wins by placing three marks in a row, horizontally, vertically, or diagonally, as the X player has in the game shown to the right. If the board fills up with neither player getting three in a row, then the game is a draw.\nBecause a skilled player can play so as never to lose, let us assume that we are playing against an imperfect player, one whose play is sometimes incorrect and allows us to win. For the moment, in fact, let us consider draws and losses to be equally bad for us.\nHow might we construct a player that will find the imperfections in its opponent’s play and learn to maximize its chances of winning?"
  },
  {
    "objectID": "intro.html#construct-decision-making-model",
    "href": "intro.html#construct-decision-making-model",
    "title": "1  Elements of Reinforcement Learning.",
    "section": "2.1 Construct Decision Making Model",
    "text": "2.1 Construct Decision Making Model\nHere is how the tic-tac-toe problem would be approached with a method making use of a value function. First we would set up a table of numbers, one for each possible state of the game. Each number will be the latest estimate of the probability of our winning from that state. We treat this estimate as the state’s value, and the whole table is the learned value function. State A has higher value than state B, or is considered “better” than state B, if the current estimate of the probability of our winning from A is higher than it is from B. Assuming we always play Xs, then for all states with three Xs in a row the probability of winning is 1, because we have already won. Similarly, for all states with three Os in a row, or that are filled up, the correct probability is 0, as we cannot win from them. We set the initial values of all the other states to 0.5, representing a guess that we have a 50% chance of winning."
  },
  {
    "objectID": "intro.html#exploratory-vs-greedy",
    "href": "intro.html#exploratory-vs-greedy",
    "title": "1  Elements of Reinforcement Learning.",
    "section": "2.2 Exploratory vs Greedy",
    "text": "2.2 Exploratory vs Greedy\nWe then play many games against the opponent. To select our moves we examine the states that would result from each of our possible moves (one for each blank space on the board) and look up their current values in the table. Most of the time we move greedily, selecting the move that leads to the state with greatest value, that is, with the highest estimated probability of winning. Occasionally, however, we select randomly from among the other moves instead. These are called exploratory moves because they cause us to experience states that we might otherwise never see."
  },
  {
    "objectID": "code_tictactoe.html#objective",
    "href": "code_tictactoe.html#objective",
    "title": "2  Excersice Code: Tic Tac Toe.",
    "section": "2.1 Objective",
    "text": "2.1 Objective\nIn the first excersice code going to make a Tic Tac Toe and check the probabilites of win."
  },
  {
    "objectID": "code_tictactoe.html#construct-tic-tac-toe-model.",
    "href": "code_tictactoe.html#construct-tic-tac-toe-model.",
    "title": "2  Excersice Code: Tic Tac Toe.",
    "section": "2.2 Construct Tic Tac Toe Model.",
    "text": "2.2 Construct Tic Tac Toe Model.\nThe Tic Tac Toe Board is a game over a grid of \\(3\\times 3\\) squares \\[\n\\left[\n\\begin{array}{ccc}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\n\\end{array}\n\\right].\n\\]\nIn each square \\((a_{ij})\\) can write a \\(\\mathbf{X}\\) or \\(\\mathbf{O}\\). A player use the \\(\\mathbf{X}\\) and the other player use the \\(\\mathbf{O}\\). In the game by turns both players place us simbols \\(\\{\\mathbf{X}, \\mathbf{O}\\}\\) in the board. The winner is the first player in obtain a three row, column or principal diagonal.\nFor example, if the first player is \\(\\mathbf{X}\\) and \\[\n    s: a_{11} = \\mathbf{X}, a_{12} = \\mathbf{O}, \\ldots \\left[\n    \\begin{array}{ccc}\n    \\mathbf{X} & \\mathbf{O} & \\mathbf{O} \\\\  \n    \\mathbf{X} & \\mathbf{X} & \\mathbf{} \\\\  \n    \\mathbf{X} & \\mathbf{} & \\mathbf{O} \\\\  \n    \\end{array}\n    \\right].\n\\]\nIn this case, the \\(\\mathbf{X}'s\\) player win. To construct the Decision Model be consider the States Set \\((\\mathcal{S})\\) as the Tic Tac Toe diagrams\n\\[\n    \\mathcal{S} =\\left\\{\\left( k, (i,j), a \\right)\\}_{k}, 1\\leq k \\leq 9, 1 \\leq i,j \\leq 3, a\\in\\{\\mathbf{X},\\mathbf{O}\\}\\right\\}.\n\\]\nThen \\(s\\in \\mathcal{S}\\)\n\\[\\begin{align*}\ns = & \\Biggl\\{\\biggl(1,\\Bigl((1,1),\\mathbf{X}\\Bigr)\\biggl),\\biggl(2,\\Bigl((1,2),\\mathbf{O}\\Bigr)\\biggl),\\biggl(3,\\Bigl((2,2),\\mathbf{X}\\Bigr)\\biggl),\\ldots \\\\\n    & \\biggl(4,\\Bigl((3,3),\\mathbf{O}\\Bigr)\\biggl),\\biggl(5,\\Bigl((3,1),\\mathbf{X}\\Bigr)\\biggl),\\biggl(6,\\Bigl((1,3),\\mathbf{O}\\Bigr)\\biggl),\\biggl(7,\\Bigl((2,1),\\mathbf{X}\\Bigr)\\biggr)\\Biggr\\}\\\\\ns & \\equiv \\left[\n    \\begin{array}{ccc}\n    \\mathbf{X} & \\mathbf{O} & \\mathbf{O} \\\\  \n    \\mathbf{X} & \\mathbf{X} & \\mathbf{} \\\\  \n    \\mathbf{X} & \\mathbf{} & \\mathbf{O} \\\\  \n    \\end{array}\n    \\right].\n\\end{align*}\\]\nNote that exists states \\(u_1,u_2,\\ldots,u_6\\) such that\n\\[\n    u_1 \\to u_2 \\to u_3 \\to \\ldots \\to u_6 \\to s.\n\\]\nIn other words, exists a function \\(f:\\mathcal{S}\\times\\mathcal{A} \\to \\mathcal{S}\\) such that\n\\[\n    S_{t+1} = f(S_t, a_t),\n\\]\nwhere \\(f\\) represent the model dynamics. In this case \\(f\\) was the response of the rival. Consider the initial condition \\(S_0\\)\n\\[\nS_0 = \\{\\} \\equiv \\left[\\begin{array}{ccc}\n&& \\\\\n&& \\\\\n&& \\\\\n\\end{array}\n\\right]\n\\]\nwhen you are the first player, if you are the second (with \\(\\mathbf{X}\\) or \\(\\mathbf{O}\\))\n\\[\n    S_0((i,j), p) = \\left(1, ((i,j), p) \\right), p \\in \\{\\mathbf{X}, \\mathbf{O}\\}.\n\\]\nIn both cases, with \\(S_0\\), the agent continue choose a action \\(a_0\\) and obtain the following state\n\\[\n    S_1 = f(S_0, a_0).\n\\]\nLater defines the history \\(H_t\\) as the sucession of the following shape\n\\[\n    H_t = \\{S_0, a_0, S_1,a_1, \\ldots, a_{t - 1}, S_t\\}\n\\]"
  },
  {
    "objectID": "dyn_prog.html#introduction",
    "href": "dyn_prog.html#introduction",
    "title": "3  Dynamical Programming",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nThe temporal structure of a typical dynamic program is\n\nAn initial state \\(X_0\\) is given\n\\(t\\leftarrow 0\\)\nwhile \\(t&lt;T\\) do\n() The controller of the system observes the current state \\(X_t\\)\nThe controller chooses an action \\(A_t\\)\n() The controller receives a reward \\(R_t\\) that depends on the current state and action.\n() the state update to \\(X_{t+1}\\)\n() \\(t\\leftarrow t + 1\\)\nend\n\nThe state \\(X_t\\) is a vector listing current values of variables deemed relevant to choosing the current action. The action \\(A_t\\) is a vector describing choices of a set of decision variables. If \\(T&lt;\\infty\\) then the problem has a finite horizon. Otherwise it is an infinite horizon problem.\nDynamic programming provides a way to maximize expected lifetime reward of a decision maker who receives a prospective reward sequence \\((R_t)_{t\\geq 0}\\) and who confronts a system that maps today’s state and control into next period’s state. A lifetime reward is an aggregation of the individual period rewards \\((R_t)_{t\\geq 0}\\) into a single value. An example of lifetime reward is an expected discounted sum\n\\[\n    E\\left[\\sum_{t\\geq 0}\\beta^t R_t\\right]\n\\]\nfor some \\(\\beta\\in (0,1)\\).\nExample A manager wants to set prices and inventories to maximize a firm’s expected present value (EPV), which, given interest rate \\(r\\), is defined as\n\\[\n    E\\left[\\sum_{k\\geq 0} \\pi_{k}\\left(\\dfrac{1}{1+r}\\right)^{k}\\right].\n\\tag{3.1}\\]\nHere \\(X_t\\), will be a vector that quantifies the size of the inventories, prices set by competitors and other factors factors relevant to profit maximization. The action \\(A_t\\) sets current prices and orders of new stock. The current reward \\(R_t\\) is current profit \\(\\pi_t\\), and the profit stream \\(\\{\\pi_t\\}_{t\\geq 0}\\) is aggregagted into a lifetime reward via Equation 3.1.\nThe core theory of dynamic programming is relative simple and concise. But implementation can be computationally demanding. That situation provides one of the major challenges facing the field of dynamic programming."
  },
  {
    "objectID": "dyn_prog.html#bellman-equations",
    "href": "dyn_prog.html#bellman-equations",
    "title": "3  Dynamical Programming",
    "section": "3.2 Bellman Equations",
    "text": "3.2 Bellman Equations\nIn this section we introduce the recursive structure of dynamic programming in a simple setting. After solving a finite-horizon model, we consider an infinite-horizon version and explain how it produces a system of nonlinear equations. Then we turn to methods for solving such systems.\n\n3.2.1 Finite-Horizon Job Search\nWe begin with a celebrated model of job search created by McCall (1970). McCall analyzed the decision problem of an unemployed worker in terms of current and prospective wage offers, impatience, and the availability of unemployment compensation.\n\n3.2.1.1 A Two Period Problem\nImagine someone who begins her working life at time \\(t=1\\) without employment. While unemployed, she receives a new job offer paying wage \\(W_t\\) at each date \\(t\\). She can accept the offer and work permanently at that wage level or reject the offer, receive unemployment compensation \\(c\\), and draw a new offer next period. We assume that the wage offer sequence is i.i.d and nonegative, with distribution \\(\\varphi\\). In particular,\n\n\\(W\\subset \\mathbb{R}^{+}\\) is a finite set of possible wage outcomes and\n\\(\\varphi:W\\to [0,1]\\) is a probability distribution on \\(W\\), assigning a probability \\(\\varphi(w)\\) to each possible wage outcome \\(w\\).\n\nThe worker is impatient. Impatiente is parametrized by a time discount factor \\(\\beta\\in (0,1)\\), so that the present value of a next-period payoff of \\(y\\) dollars is \\(\\beta_y\\). Since \\(\\beta &lt; 1\\), the worker will be tempted to accept reasonable offeres, rather than to wait for better ones. A key question is how long to wait.\nSuppose as a first step that working life is just two periods. To solve our problem we work backwards, starting at the final date \\(t=2\\) after \\(W_2\\) has been observed.\nIf she is already employed, the worker has no decision to make: she continues working at her current wage. If she is unemployed, then she should take the largest of \\(c\\) and \\(W_2\\).\nNow we step back to \\(t = 1\\). At this time, having received offer \\(W_1\\), the unemployed worker’s options are (a) accept \\(W_1\\) and receive it in both periods or (b) reject it, receive unemployment compensation \\(c\\), and then, in the second period, choose the maximum of \\(W_2\\) and \\(c\\).\nLet’s assume that the worker seeks to maximize expected present value. The EPV of option (a) is \\(W_1 + \\beta W_1\\), which is also called the stopping value. The EPV of option (b), also called the continuation value, is \\(h_1 := c +\\beta E\\left[\\max\\{c, W_2\\}\\right]\\). More explicitly,\n\\[\n    h_1 = c + \\beta \\sum_{w'\\in W}v_2(w')\\varphi(w'), \\text{where }v_2(w) := \\max\\{c,w\\}.\n\\tag{3.2}\\]\nThe optimal choice as \\(t=1\\) is now clear: accept the offer if \\(W_1+\\beta W_1 \\geq h\\) and reject otherwise.\n\n\n3.2.1.2 Value Functions\nA key idea in dynamic programming is to use value functions to track maximal lifetime rewards from a given state at a given time. The time 2 value function \\(v_2\\) in Equation 3.2 returns the maximum value obtained in the final stage for each possible realization of the time 2 wage offer. The time 1 value function \\(v_1\\) evaluated at \\(w\\in W\\) is\n\\[\n      v_1 := \\max\\{w + \\beta w, c + \\beta \\sum_{w'\\in W}v_2(w)\\varphi(w')\\}\n\\tag{3.3}\\]\nIt represents the present value of expected lifetime income after receiving the first offer \\(w\\), conditional conditional on choosing optimally in both periods.\nNow calculate the \\(w\\) such that solves the indifference condition\n\\[\n    w + \\beta w = c + \\beta \\sum_{w'\\in W}v_2(w')\\varphi(w')\n\\]\nThis is,\n\\[\\begin{align*}\n    w = & \\dfrac{c + \\beta \\sum_{w'\\in W}v_2(w')\\varphi(w')}{1 + \\beta}\\\\\n      = & \\dfrac{h_1}{1 + \\beta}\n\\end{align*}\\]"
  }
]